{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) How would you define Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input ve Outputu  değerlendirerek aralarında anlamlı bir bağ, ilişki bularak diğer inputlar üzerinde de doğru,anlamlı output verebilen programlar oluşturabilen algoritma sistemleridir. Büyük veri yığınlarıyla uğraşılıyorsa, geçmişe dayalı tahminlerden elde edilecek sonuçlar önem arz ediyorsa ya da aynı işlemlerin çok defa yapılması gerekiyorsa ML kullanılmalıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 2) What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İncelenmesi gereken veriler numerik veriler ise Supervised Learning(denetimli), kategorik ise Unsupervised Learning(denetimsiz) kullanılmalıdır. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning örnekleri :\n",
    "    1. Classifikation\n",
    "    2. Regression\n",
    "    3. Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised Learning örnekleri : \n",
    "    1.Clustering\n",
    "    2.Association \n",
    "    3.Overlapping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) What are the test and validation set, and why would you want to use them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İncelenen veri üzerinde diğer inputların çıktılarını da doğru tahmin edebilmesi için önce verilerin yüksek bir kısmında(örneğin %70) öğretme işlemi yapılması gerekir. Öğretilen bu kısmın içerisinde öğretme yine train verisi içerisindeki bir parça ile kalan veri ile test eldilmeye  geçmeden denenir.Bu train verisi içerinde test edilen kısım validation settir. Train ve validation işlemlerinden sonra toplam verinin kalan kısmı(örneğin %30) ile öğretilen işlem test edilir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing adımları:\n",
    "   1. Dublicated values : Veri seti içerisinde aynı değerlere sahip veriler varsa algoritmanın doğru sonuçlanması için bunların temizlenmesi gerekir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Imbalanced data : İncelenen veri üzerinde bir dengesizlik varsa örneğin oransal bir dengesizlik bu algoritmanın doğru öğrenememesine sebep olur. Örneğin nadir görülen hastalıkların tespitinde rastlanılabilir. Nadir hastalık görülen birey sayısı bir veri setinde görülmeyen bireylere göre çok azdır. Bu da makine öğrenmesinin genellemelerde yanlış sonuç vermesine sebep olabilir. Bu nedenle bu veriler dengelenmelidir. Bu denge 2 farklı yol ile sağlanabilir. Birinci yol Undersampling yapılarak çok olan veriyi nadir görülen verinin değerine yaklaştırılabilir ya da ikinci yol olarak Oversampling yapılan nadir olan değer çok olan değer seviyesine yükseltilebilir.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Missing values: Veri seti içerisindeki bazı veriler girilmemiş ya da değeri olmadığı için \"?\",\"none\" gibi ifadelerle doldurulmuş olabilir. Fakat bu doldurmalar öğrenme sürecini olumsuz etkileyeceğinden dolayı düzeltmek gerekir. Düzeltmek için bu eksik/kayıp veriler silinebilir. Veri seti sürekli değişkenlerden oluşuyorsa ortalama veya medyan değerleriyle de doldurulabilir.Veri seti normal dağılıma sahip ise ortalama değerleriyle doldurmak daha doğrudur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Outlier detection : Çeşitli bozunumlar sonucunda veri setinde anomali durumları gözlenebilir. Örneğin hava sıcaklığının 2500 derece olması gibi. Bu sorunun giderilmesi için 3 farklı yöntem uygulanabilir. İlk çözüm standart deviation uygulamak olabilir. Yani incelenen veri yaklaşık normal dağılıma sahip ise verinin %68'i standart sapma içerisinde bulunur. Standart sapmanın 3 katından fazla bir değer varsa bu değerler yüksek ihtimal ile outlier değerlerdir bu değerler silinir. İkinci çözüm olarak IOR calculation yapılabilir. Yani üçüncü çeyrekten 1,5IQR fazla olan değerler ve birinci çeyrekten 1,5IQR az olan değerler outlier olarak kabul edilerek veri setinden çıkartılır. Üçüncü yöntem olarak ise Isolation forest yöntemi kullanılabilir.Anormalliklerin bir veri setinde azınlık olarak açıkca görülmesi düşüncesi temelinde çalışan başarılı bir karar ağaçlarına ait bir unsupervised learning algoritmasıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Feature scaling : ML algoritmaları sayı temelli olduğundan dolayı 2 farklı feature arasında belirgin bir sayısal üstünlük varsa yüksek değerli olanın daha önemli olduğunu düşünür. Bu durumu engellemek için 2 farklı yöntem vardır. İlk yöntem standardization ile ortalama 0 standart sapma 1 haline getirilir. İkinci yöntem olan normalization ile değleri 0 ile 1 arasına çekildiği bir değerlendirme biçimidir. Ayrıca incelenen veri sürekli veri ise bucketing kullanılarak bu veri kategorik veri gibi değerlendirip verideki aşırı değerler dengeli olarak gruplandırılarak görselleştirilebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Feature extraction : Farklı özellikleri birleştirerek elde edilmek istenen sonuç için daha yararlı olabilecek bir yeni bir özellik elde edip özellik azaltmaya yarayan yöntemdir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Feature encoding : Temelde kategorik veriyi sürekli veriye dönüştürerek modelde kullanma işlemidir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Train/validation/test split : Makine öğrenmesi modelleri eğitilirken veri setinin farklı parçalara bölünmesi gerekir. Train bölümü veri setinin büyük bölümünü kapsamalıdır. Bu bölümde algoritma fit etme işlemini gerçekleştirir. Her zaman kullanılmasa da validation işlemi ile train verileri içerisinden bir kısım ile test kısmına geçmeden önce denemeler yapılır. Fakat validation işlemi overfitting durumuna sebep olabilir bunu engellemek için cross validation kullanılabilir. Bu yöntem train üzerinde farklı splitler üzerinde denemeler yaparak test kısmına geçer. Test bölümünde ise veri setinde algoritmanın öğrenmediği kısım üzerinde test edilip öğrenmenin ne derece doğru sonuç verdiği irdelenebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri setinden doğru çalışan bir program elde edebilmek için verinin amaca uygun olarak temizlenerek, kullanılacak yönteme hazır hale getirilmesi gerekir. Bu nedenle veri öncelikle preprocessing işlemine tabii tutulur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) How you can explore and analyse countionus and discrete variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countionus değişkenler bir aralıktaki sayıları belirtir. Günlük rüzgar hızı sürekli veriye örnek olarak verilir. Discrete değişkenler somut sayıları ifade eder. Her ay marketten alınan ürün sayısı discrete değişkenlere örnek verilebilir. Bu veriler çeşitli grafilkler ile keşfedilip, analiz edilebilir. Countionus değişkenler çizgi grafiği ile görselleştirilip, analiz edilebilirken discrete değişkenler bir dağılım grafiği yardımıyla görselleştirilerek incelenebilir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Analyse the plot given below. (What is the plot and variable type, check the distribution and make comment about how you can preproccess it.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soruda verilen grafik bir histogramdır. İncelenen sürekli verinin frekansları üzerinden yorum yapılmasını sağlayan grafiktir. Buradan yola çıkılarak veri contionus değişkenlere sahiptir. Veri normal bir dağılım göstermez. Standart preprocess işlemleri yapılır. Tekrar eden veriler olup olmadığına bakılır. Kayıp veriler varsa bunlar medyan değerleri ile doldurulmalıdır, normal dağılım göstermediği için. Bu verideki ayırt edici olan nokta genişlik değeri 0,8 gibi bir noktada ayrı bir özellik gösterir. Bu ayrılan nokta tam olarak tespit edilebilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
